{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tox21 데이터 집합\n",
    "# 이 데이터 집합에는 1만 개 분자가 안드로겐 수용체와 상호작용하는지 실험한 결과가 들어있다.\n",
    "# 새로운 분자가 주어졌을 때 안드로겐 수용체와 상호작용할지 여부를 예측한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import deepchem as dc\n",
    "\n",
    "_, (train, valid, test), _ = dc.molnet.load_tox21()\n",
    "\n",
    "# X : 피처 벡터, y : 정답(1/0), w : 예제 가중치\n",
    "# w에는 양수 데이터를 더 강조하는 데 사용할 수 있는 데이터별 권장 가중치가 들어 있다\n",
    "# 적은 데이터의 중요도를 늘리는 것은 불균형한 데이터 집합을 처리할 때 흔히 사용하는 기술\n",
    "train_X, train_y, train_w = train.X, train.y, train.w\n",
    "valid_X, valid_y, valid_w = valid.X, valid.y, valid.w\n",
    "test_X, test_y, test_w = test.X, test.y, test.w\n",
    "\n",
    "# 불필요한 데이터 집합 삭제\n",
    "train_y = train_y[:, 0]\n",
    "valid_y = valid_y[:, 0]\n",
    "test_y = test_y[:, 0]\n",
    "train_w = train_w[:, 0]\n",
    "valid_w = valid_w[:, 0]\n",
    "test_w = test_w[:, 0]\n",
    "\n",
    "# 드롭아웃 확률 플레이스홀더 추가\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# 완전연결 아키텍처 정의\n",
    "# 다양한 크기의 미니배치를 받을 수 있는 플레이스홀더 정의\n",
    "d = 1024  # 피처 벡터의 차원\n",
    "with tf.name_scope(\"placeholders\") :\n",
    "    x = tf.placeholder(tf.float32, (None, d))\n",
    "    y = tf.placeholder(tf.float32, (None,))\n",
    "\n",
    "# 은닉층 정의\n",
    "with tf.name_scope(\"hidden_layer\") :\n",
    "    W = tf.Variable(tf.random_normal((d, n_hidden)))  # 행렬 형태\n",
    "    b = tf.Variable(tf.random_normal((n_hidden,)))\n",
    "    # 활성화 함수\n",
    "    x_hidden = tf.nn.relu(tf.matmul(x, W) + b)\n",
    "    # 드롭아웃 적용\n",
    "    x_hidden = tf.nn.dropout(x_hidden, keep_prob)\n",
    "    \n",
    "with tf.name_scope(\"output\") :\n",
    "    W = tf.Variable(tf.random_normal((n_hidden, 1)))\n",
    "    b = tf.Variable(tf.random_normal((1,)))\n",
    "    y_logit = tf.matmul(x_hidden, W) + b\n",
    "    # 시그모이드는 클래스 1의 확률을 반환\n",
    "    y_one_prob = tf.sigmoid(y_logit)\n",
    "    # P(y=1)을 반올림해서 정확한 예측값을 구한다\n",
    "    y_pred = tf.round(y_one_prob)\n",
    "\n",
    "with tf.name_scope(\"loss\") :\n",
    "    # 각 데이터에 대해 교차 엔트로피 항을 계산\n",
    "    y_expand = tf.expand_dims(y, 1)\n",
    "    entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels = y_expand)\n",
    "    # 모든 기여를 더한다\n",
    "    l = tf.reduce_sum(entropy)\n",
    "\n",
    "with tf.name_scope(\"optim\") :\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(l)\n",
    "\n",
    "with tf.name_scope(\"summaries\") :\n",
    "    tf.summary.scalar(\"loss\", l)\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "# 미니배치 학습\n",
    "\n",
    "# 미니배치를 구현하려면 sess.run을 호출할 때마다\n",
    "# 미니배치 크기만큼의 데이터를 뽑아야 한다\n",
    "\n",
    "step = 0\n",
    "for epoch in range(n_epochs) :\n",
    "    pos = 0\n",
    "    while pos < N :\n",
    "        batch_X = train_X[pos : pos+batch_size]\n",
    "        batch_y = train_y[pos : pos+batch_size]\n",
    "        feed_dict = {x : batch_X, y : batch_y, keep_prob : dropout_prob}\n",
    "        _, summary, loss = sess.run([train_op, merged, l], feed_dict = feed_dict)\n",
    "        print(\"epoch %d, step %d, loss : %f\" % (epoch, step, loss))\n",
    "        train_writer.add_summary(summary, step)\n",
    "        \n",
    "        step += 1\n",
    "        pos += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# np.random.seed(456)\n",
    "# import  tensorflow as tf\n",
    "# tf.set_random_seed(456)\n",
    "# import matplotlib.pyplot as plt\n",
    "# import deepchem as dc\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# _, (train, valid, test), _ = dc.molnet.load_tox21()\n",
    "# train_X, train_y, train_w = train.X, train.y, train.w\n",
    "# valid_X, valid_y, valid_w = valid.X, valid.y, valid.w\n",
    "# test_X, test_y, test_w = test.X, test.y, test.w\n",
    "\n",
    "# # Remove extra tasks\n",
    "# train_y = train_y[:, 0]\n",
    "# valid_y = valid_y[:, 0]\n",
    "# test_y = test_y[:, 0]\n",
    "# train_w = train_w[:, 0]\n",
    "# valid_w = valid_w[:, 0]\n",
    "# test_w = test_w[:, 0]\n",
    "\n",
    "\n",
    "# # Generate tensorflow graph\n",
    "# d = 1024\n",
    "# n_hidden = 50\n",
    "# learning_rate = .001\n",
    "# n_epochs = 10\n",
    "# batch_size = 100\n",
    "\n",
    "# with tf.name_scope(\"placeholders\"):\n",
    "#   x = tf.placeholder(tf.float32, (None, d))\n",
    "#   y = tf.placeholder(tf.float32, (None,))\n",
    "# with tf.name_scope(\"hidden-layer\"):\n",
    "#   W = tf.Variable(tf.random_normal((d, n_hidden)))\n",
    "#   b = tf.Variable(tf.random_normal((n_hidden,)))\n",
    "#   x_hidden = tf.nn.relu(tf.matmul(x, W) + b)\n",
    "# with tf.name_scope(\"output\"):\n",
    "#   W = tf.Variable(tf.random_normal((n_hidden, 1)))\n",
    "#   b = tf.Variable(tf.random_normal((1,)))\n",
    "#   y_logit = tf.matmul(x_hidden, W) + b\n",
    "#   # the sigmoid gives the class probability of 1\n",
    "#   y_one_prob = tf.sigmoid(y_logit)\n",
    "#   # Rounding P(y=1) will give the correct prediction.\n",
    "#   y_pred = tf.round(y_one_prob)\n",
    "# with tf.name_scope(\"loss\"):\n",
    "#   # Compute the cross-entropy term for each datapoint\n",
    "#   y_expand = tf.expand_dims(y, 1)\n",
    "#   entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y_expand)\n",
    "#   # Sum all contributions\n",
    "#   l = tf.reduce_sum(entropy)\n",
    "\n",
    "# with tf.name_scope(\"optim\"):\n",
    "#   train_op = tf.train.AdamOptimizer(learning_rate).minimize(l)\n",
    "\n",
    "# with tf.name_scope(\"summaries\"):\n",
    "#   tf.summary.scalar(\"loss\", l)\n",
    "#   merged = tf.summary.merge_all()\n",
    "\n",
    "# train_writer = tf.summary.FileWriter('/tmp/fcnet-tox21',\n",
    "#                                      tf.get_default_graph())\n",
    "# N = train_X.shape[0]\n",
    "# with tf.Session() as sess:\n",
    "#   sess.run(tf.global_variables_initializer())\n",
    "#   step = 0\n",
    "#   for epoch in range(n_epochs):\n",
    "#     pos = 0\n",
    "#     while pos < N:\n",
    "#       batch_X = train_X[pos:pos+batch_size]\n",
    "#       batch_y = train_y[pos:pos+batch_size]\n",
    "#       feed_dict = {x: batch_X, y: batch_y}\n",
    "#       _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n",
    "#       print(\"epoch %d, step %d, loss: %f\" % (epoch, step, loss))\n",
    "#       train_writer.add_summary(summary, step)\n",
    "    \n",
    "#       step += 1\n",
    "#       pos += batch_size\n",
    "\n",
    "#   # Make Predictions\n",
    "#   valid_y_pred = sess.run(y_pred, feed_dict={x: valid_X})\n",
    "\n",
    "# score = accuracy_score(valid_y, valid_y_pred)\n",
    "# print(\"Unweighted Classification Accuracy: %f\" % score)\n",
    "\n",
    "# weighted_score = accuracy_score(valid_y, valid_y_pred, sample_weight=valid_w)\n",
    "# print(\"Weighted Classification Accuracy: %f\" % weighted_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100\n",
      "epoch 0, step 0\n",
      "100 200\n",
      "epoch 0, step 1\n",
      "200 300\n",
      "epoch 0, step 2\n",
      "300 400\n",
      "epoch 0, step 3\n",
      "400 500\n",
      "epoch 0, step 4\n",
      "500 600\n",
      "epoch 0, step 5\n",
      "600 700\n",
      "epoch 0, step 6\n",
      "700 800\n",
      "epoch 0, step 7\n",
      "800 900\n",
      "epoch 0, step 8\n",
      "900 1000\n",
      "epoch 0, step 9\n",
      "0 100\n",
      "epoch 1, step 10\n",
      "100 200\n",
      "epoch 1, step 11\n",
      "200 300\n",
      "epoch 1, step 12\n",
      "300 400\n",
      "epoch 1, step 13\n",
      "400 500\n",
      "epoch 1, step 14\n",
      "500 600\n",
      "epoch 1, step 15\n",
      "600 700\n",
      "epoch 1, step 16\n",
      "700 800\n",
      "epoch 1, step 17\n",
      "800 900\n",
      "epoch 1, step 18\n",
      "900 1000\n",
      "epoch 1, step 19\n",
      "0 100\n",
      "epoch 2, step 20\n",
      "100 200\n",
      "epoch 2, step 21\n",
      "200 300\n",
      "epoch 2, step 22\n",
      "300 400\n",
      "epoch 2, step 23\n",
      "400 500\n",
      "epoch 2, step 24\n",
      "500 600\n",
      "epoch 2, step 25\n",
      "600 700\n",
      "epoch 2, step 26\n",
      "700 800\n",
      "epoch 2, step 27\n",
      "800 900\n",
      "epoch 2, step 28\n",
      "900 1000\n",
      "epoch 2, step 29\n",
      "0 100\n",
      "epoch 3, step 30\n",
      "100 200\n",
      "epoch 3, step 31\n",
      "200 300\n",
      "epoch 3, step 32\n",
      "300 400\n",
      "epoch 3, step 33\n",
      "400 500\n",
      "epoch 3, step 34\n",
      "500 600\n",
      "epoch 3, step 35\n",
      "600 700\n",
      "epoch 3, step 36\n",
      "700 800\n",
      "epoch 3, step 37\n",
      "800 900\n",
      "epoch 3, step 38\n",
      "900 1000\n",
      "epoch 3, step 39\n",
      "0 100\n",
      "epoch 4, step 40\n",
      "100 200\n",
      "epoch 4, step 41\n",
      "200 300\n",
      "epoch 4, step 42\n",
      "300 400\n",
      "epoch 4, step 43\n",
      "400 500\n",
      "epoch 4, step 44\n",
      "500 600\n",
      "epoch 4, step 45\n",
      "600 700\n",
      "epoch 4, step 46\n",
      "700 800\n",
      "epoch 4, step 47\n",
      "800 900\n",
      "epoch 4, step 48\n",
      "900 1000\n",
      "epoch 4, step 49\n",
      "0 100\n",
      "epoch 5, step 50\n",
      "100 200\n",
      "epoch 5, step 51\n",
      "200 300\n",
      "epoch 5, step 52\n",
      "300 400\n",
      "epoch 5, step 53\n",
      "400 500\n",
      "epoch 5, step 54\n",
      "500 600\n",
      "epoch 5, step 55\n",
      "600 700\n",
      "epoch 5, step 56\n",
      "700 800\n",
      "epoch 5, step 57\n",
      "800 900\n",
      "epoch 5, step 58\n",
      "900 1000\n",
      "epoch 5, step 59\n",
      "0 100\n",
      "epoch 6, step 60\n",
      "100 200\n",
      "epoch 6, step 61\n",
      "200 300\n",
      "epoch 6, step 62\n",
      "300 400\n",
      "epoch 6, step 63\n",
      "400 500\n",
      "epoch 6, step 64\n",
      "500 600\n",
      "epoch 6, step 65\n",
      "600 700\n",
      "epoch 6, step 66\n",
      "700 800\n",
      "epoch 6, step 67\n",
      "800 900\n",
      "epoch 6, step 68\n",
      "900 1000\n",
      "epoch 6, step 69\n",
      "0 100\n",
      "epoch 7, step 70\n",
      "100 200\n",
      "epoch 7, step 71\n",
      "200 300\n",
      "epoch 7, step 72\n",
      "300 400\n",
      "epoch 7, step 73\n",
      "400 500\n",
      "epoch 7, step 74\n",
      "500 600\n",
      "epoch 7, step 75\n",
      "600 700\n",
      "epoch 7, step 76\n",
      "700 800\n",
      "epoch 7, step 77\n",
      "800 900\n",
      "epoch 7, step 78\n",
      "900 1000\n",
      "epoch 7, step 79\n",
      "0 100\n",
      "epoch 8, step 80\n",
      "100 200\n",
      "epoch 8, step 81\n",
      "200 300\n",
      "epoch 8, step 82\n",
      "300 400\n",
      "epoch 8, step 83\n",
      "400 500\n",
      "epoch 8, step 84\n",
      "500 600\n",
      "epoch 8, step 85\n",
      "600 700\n",
      "epoch 8, step 86\n",
      "700 800\n",
      "epoch 8, step 87\n",
      "800 900\n",
      "epoch 8, step 88\n",
      "900 1000\n",
      "epoch 8, step 89\n",
      "0 100\n",
      "epoch 9, step 90\n",
      "100 200\n",
      "epoch 9, step 91\n",
      "200 300\n",
      "epoch 9, step 92\n",
      "300 400\n",
      "epoch 9, step 93\n",
      "400 500\n",
      "epoch 9, step 94\n",
      "500 600\n",
      "epoch 9, step 95\n",
      "600 700\n",
      "epoch 9, step 96\n",
      "700 800\n",
      "epoch 9, step 97\n",
      "800 900\n",
      "epoch 9, step 98\n",
      "900 1000\n",
      "epoch 9, step 99\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "N = 947\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    pos = 0\n",
    "    while pos < N:\n",
    "        print(pos, pos+batch_size)\n",
    "        # batch_X = train_X[pos:pos+batch_size]\n",
    "        # batch_y = train_y[pos:pos+batch_size]\n",
    "        print(\"epoch %d, step %d\" % (epoch, step))\n",
    "\n",
    "        step += 1\n",
    "        pos += batch_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
