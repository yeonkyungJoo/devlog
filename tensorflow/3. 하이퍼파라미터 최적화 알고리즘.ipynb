{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 기준선 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# np.random.seed(456)\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# import deepchem as dc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tox21 데이터셋에 대한 랜덤 포레스트 정의하고 학습하기\n",
    "\n",
    "_, (train, valid, test), _ = dc.molnet.load_tox21()\n",
    "train_X, train_y, train_w = train.X, train.y, train.w\n",
    "valid_X, valid_y, valid_w = valid.X, valid.y, valid.w\n",
    "test_X, test_y, test_w = test.X, test.y, test.w\n",
    "\n",
    "# Remove extra tasks\n",
    "train_y = train_y[:, 0]\n",
    "valid_y = valid_y[:, 0]\n",
    "test_y = test_y[:, 0]\n",
    "train_w = train_w[:, 0]\n",
    "valid_w = valid_w[:, 0]\n",
    "test_w = test_w[:, 0]\n",
    "\n",
    "# 텐서플로 그래프 생성하기\n",
    "sklearn_model = RandomForestClassifier(class_weight='balanced', n_estimators=50)\n",
    "\n",
    "print(\"About to fit model on training set.\")\n",
    "sklearn_model.fit(train_X, train_y)\n",
    "\n",
    "train_y_pred = sklearn_model.predict(train_X)\n",
    "valid_y_pred = sklearn_model.predict(valid_X)\n",
    "test_y_pred = sklearn_model.predict(test_X)\n",
    "\n",
    "weighted_score = accuracy_score(train_y, train_y_pred, sample_weight=train_w)\n",
    "print(\"Weighted train Classification Accuracy: %f\" % weighted_score)\n",
    "weighted_score = accuracy_score(valid_y, valid_y_pred, sample_weight=valid_w)\n",
    "print(\"Weighted valid Classification Accuracy: %f\" % weighted_score)\n",
    "weighted_score = accuracy_score(test_y, test_y_pred, sample_weight=test_w)\n",
    "print(\"Weighted test Classification Accuracy: %f\" % weighted_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 대학원생 하강법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터를 서로 다른 Tox21 완전연결 네트워크로 매핑하는 함수\n",
    "def eval_tox21_hyperparams(\n",
    "    n_hidden = 50,           # 각각의 은닉층에 있는 뉴런의 개수\n",
    "    n_layers = 1,            # 은닉층의 개수\n",
    "    learning_rate = 0.001,   # 경사 하강법 률\n",
    "    dropout_prob = 0.5,      # 학습 단계에서 뉴런이 드롭되지 않을 확률\n",
    "    n_epochs = 45,           # 전체 데이터 사용 횟수\n",
    "    batch_size = 100,        # 각 배치에서 데이터포인트 수\n",
    "    weight_positives = True\n",
    ") :\n",
    "    print(\"---------------------------------------------\")\n",
    "    print(\"Model hyperparameters\")\n",
    "    print(\"n_hidden = %d\" % n_hidden)\n",
    "    print(\"n_layers = %d\" % n_layers)\n",
    "    print(\"learning_rate = %f\" % learning_rate)\n",
    "    print(\"n_epochs = %d\" % n_epochs)\n",
    "    print(\"batch_size = %d\" % batch_size)\n",
    "    print(\"weight_positives = %s\" % str(weight_positives))\n",
    "    print(\"dropout_prob = %f\" % dropout_prob)\n",
    "    print(\"---------------------------------------------\")\n",
    "    \n",
    "    d = 1024\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default() :\n",
    "        _, (train, valid, test), _ = dc.molnet.load_tox21()\n",
    "        train_X, train_y, train_w = train.X, train.y, train.w\n",
    "        valid_X, valid_y, valid_w = valid.X, valid.y, valid.w\n",
    "        test_X, test_y, test_w = test.X, test.y, test.w\n",
    "        \n",
    "        # Remove extra tasks\n",
    "        train_y = train_y[:, 0]\n",
    "        valid_y = valid_y[:, 0]\n",
    "        test_y = test_y[:, 0]\n",
    "        train_w = train_w[:, 0]\n",
    "        valid_w = valid_w[:, 0]\n",
    "        test_w = test_w[:, 0]\n",
    "        \n",
    "        # Generate tersorflow graph\n",
    "        with tf.name_scope(\"placeholders\") :\n",
    "            x = tf.placeholder(tf.float32, (None, d))\n",
    "            y = tf.placeholder(tf.float32, (None,))\n",
    "            w = tf.placeholder(tf.float32, (None,))\n",
    "            keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        for layer in range(n_layers) :\n",
    "            with tf.name_scope(\"layer-%d\" % layer) :\n",
    "                W = tf.Variable(tf.random_normal((d, n_hidden)))\n",
    "                b = tf.Variable(tf.random_normal((n_hidden,)))\n",
    "                x_hidden = tf.nn.relu(tf.matmul(x, W)+b)\n",
    "                # Apply dropout\n",
    "                x_hidden = tf.nn.dropout(x_hidden, keep_prob)\n",
    "        \n",
    "        with tf_name_scope(\"output\") :\n",
    "            W = tf.Variable(tf.random_normal((n_hidden, 1)))\n",
    "            b = tf.Variable(tf.random_normal((1,)))\n",
    "            y_logit = tf.matmul(x_hidden, W) + b\n",
    "            # the sigmoid gives the class probability of 1\n",
    "            y_one_prob = tf.sigmoid(y_logit)\n",
    "            y_pred = tf.round(y_one_prob)\n",
    "            \n",
    "        with tf.name_scope(\"loss\") :\n",
    "            y_expand = tf.expand_dims(y, 1)\n",
    "            entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y_expand)\n",
    "            # 가중치 곱하기\n",
    "            if weight_positives :\n",
    "                w_expand = tf.expand_dims(w, 1)\n",
    "                entropy = w_expand * entropy\n",
    "            # Sum all contributions\n",
    "            l = tf.reduce_sum(entropy)\n",
    "        \n",
    "        with tf.name_scope(\"optim\") :\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate).minimize(l)\n",
    "            \n",
    "        with tf.name_scope(\"summaries\") :\n",
    "            tf.summary.scalar(\"loss\", l)\n",
    "            merged = tf.summary.merge_all()\n",
    "            \n",
    "        hyperparam_str = \"d-%d-hidden-%d-lr-%f-n_epochs-%d-batch_size-%d-weight_pos-%s\" % (\n",
    "        d, n_hidden, learning_rate, n_epochs, batch_size, str(weight_positives))\n",
    "        train_writer = tf.summary.FileWriter('/tmp/fcnet-func-'+ hyperparam_str, tf.get_default_graph())\n",
    "        \n",
    "        N = train_X.shape[0]\n",
    "        with tf.Session() as sess :\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            step = 0\n",
    "            for epoch in range(n_epochs) :\n",
    "                pos = 0\n",
    "                batch_X = train_X[pos:pos+batch_size]\n",
    "                batch_y = train_y[pos:pos+batch_size]\n",
    "                batch_w = train_w[pos:pos+batch_size]\n",
    "                feed_dict = { x : batch_X, \n",
    "                             y : batch_y, \n",
    "                             w : batch_w,\n",
    "                             keep_prob = dropout_prob}\n",
    "                _, summary, loss = sess.run([train_op, merged, l], \n",
    "                                            feed_dict=feed_dict)\n",
    "                print(\"epoch %d, step %d, loss : %f\" %(epoch, step, loss))\n",
    "                train_writer.add_summary(summary, step)\n",
    "                \n",
    "                step += 1\n",
    "                pos += batch_size\n",
    "                \n",
    "            # Make Predictions (set keep_prob to 1.0 for predictions)\n",
    "            valid_y_pred = sess.run(y_pred, feed_dict = {x : valid_x, keep_prob : 1.0})\n",
    "        \n",
    "        weighted_score = accuracy_score(valid_y, valid_y_pred, sample_weight=valid_w)\n",
    "        print(\"Valid Weighted Classification Accuracy : %f\" % weighted_score)\n",
    "    return weighted_score            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 그리드 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tox21 완전연결 네트워크 하이퍼파라미터에 대한 그리드 탐색 수정하기\n",
    "scores = {}\n",
    "n_reps = 3\n",
    "hidden_sizes = [50]\n",
    "epochs = [10]\n",
    "dropouts = [.5, 1.0]\n",
    "num_layers = [1, 2]\n",
    "\n",
    "for rep in range(n_reps) :\n",
    "    for n_epochs in epochs :\n",
    "        for hidden_size in hidden_sizes :\n",
    "            for dropout in dropouts :\n",
    "                for n_layers in num_layers :\n",
    "                    score = eval_tox21_hyperparams( n_hidden = hidden_size, \n",
    "                                                   n_epochs = n_epochs, \n",
    "                                                   dropout_prob = dropout, \n",
    "                                                   n_layers = n_layers)\n",
    "                    if (hidden_size, n_epochs, dropout, n_layers) not in scores :\n",
    "                        scores[(hidden_size, n_epochs, dropout, n_layers)] = []\n",
    "                    scores[(hidden_size, n_epochs, dropout, n_layers)].append(score)\n",
    "                    \n",
    "print(\"All scores\")\n",
    "print(scores)\n",
    "\n",
    "avg_scores = {}\n",
    "for params, param_scores in scores.iteritems() :\n",
    "    avg_scores[params] = np.mean(np.array(param_scores))\n",
    "print(\"Scores Averaged over %d repetitions\" % n_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 랜덤 하이퍼파라미터 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.94409143e-03 4.10510167e-05 6.61378432e-02 2.39241922e-05\n",
      " 1.25530682e-02]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 학습률 샘플링하기\n",
    "n_rates = 5\n",
    "learning_rate = 10**(-np.random.uniform(low=1, high=6, size=n_rates))\n",
    "print(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
